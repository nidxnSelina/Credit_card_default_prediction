---
title: "Project: Credit card default prediction"
output: html_notebook
---

Feb.13th 2024
Chenbo Huang, Jiayi Wu, Selina Wang, Yiting Hu, Ziao Xiong

# 1. Introduction

1.1 Problem description
Predicting whether a client will default on their credit card payment has been a significant part of risk assessment for credit card companies. In our project, we used 8 models to predict the case of default based on a collection of demographic, repayment status and amount records of clients in Taiwan from April to September 2005.

1.2 Data description
The dataset is acquired from Kaggle: https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset?resource=download

This dataset contains information on default payments, demographic factors, history of repayment status, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. It contains records of 30000 customers with 25 features. Below is a description of them:

- ID: ID of each client
- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)
- SEX: Gender (1 = male, 2 = female)
- EDUCATION: (0 = others, 1 = graduate school, 2 = university, 3 = high school, 4 = others, 5 = unknown, 6 = unknown)
- MARRIAGE: Marital status (0 = others, 1 = married, 2 = single, 3 = others)
- AGE: Age in years
Scale of PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, and PAY_6: -2 = no consumption, -1 = paid in full, 0 = use of revolving credit (paid minimum only), 1 = payment delay for one month, 2 = payment delay for two months, â€¦, 8 = payment delay for eight months, 9 = payment delay for nine months and above
- PAY_0: Repayment status in September, 2005 
- PAY_2: Repayment status in August, 2005  
- PAY_3: Repayment status in July, 2005  
- PAY_4: Repayment status in June, 2005  
- PAY_5: Repayment status in May, 2005  
- PAY_6: Repayment status in April, 2005  
All amount of bill statement are shown in New Taiwan dollar (NT dollar)
- BILL_AMT1: Amount of bill statement in September, 2005  
- BILL_AMT2: Amount of bill statement in August, 2005  
- BILL_AMT3: Amount of bill statement in July, 2005  
- BILL_AMT4: Amount of bill statement in June, 2005  
- BILL_AMT5: Amount of bill statement in May, 2005  
- BILL_AMT6: Amount of bill statement in April, 2005  
- PAY_AMT1: Amount of previous payment in September, 2005  
- PAY_AMT2: Amount of previous payment in August, 2005  
- PAY_AMT3: Amount of previous payment in July, 2005  
- PAY_AMT4: Amount of previous payment in June, 2005  
- PAY_AMT5: Amount of previous payment in May, 2005  
- PAY_AMT6: Amount of previous payment in April, 2005  
- default.payment.next.month: Default payment (0 = no, 1 = yes)


# 2. Exploratory Data Analysis

```{r}
ccard = read.csv("/Users/selina.wang/Desktop/Junior Spring/DAT500S final project/Credit_card_default_prediction/UCI_Credit_Card.csv")
attach(ccard)
str(ccard)
```

Explore the numerical variables: LIMIT_BAL and AGE
```{r}
par(mfrow=c(1,2))

boxplot(LIMIT_BAL~default.payment.next.month, xlab = "Defaulter", col = "steelblue")
boxplot(AGE~default.payment.next.month, xlab = "Defaulter", col = "steelblue")

par(mfrow=c(1,1))
```

Explore the categorical variables: SEX, EDUCATION, MARRIAGE
```{r}
library(ggplot2)
library(dplyr)

categorical_features <- c('SEX', 'EDUCATION', 'MARRIAGE')
ccard_cat <- ccard[categorical_features]
ccard_cat$Defaulter <- ccard$default.payment.next.month

# Replace values with labels
ccard_cat$SEX <- factor(ccard_cat$SEX, labels = c("Male", "Female"))
ccard_cat$EDUCATION <- factor(ccard_cat$EDUCATION, levels = c(1, 2, 3, 4), labels = c("Graduate school", "University", "High school", "Others"))
ccard_cat$MARRIAGE <- factor(ccard_cat$MARRIAGE, levels = c(1, 2, 3), labels = c("Married", "Single", "Others"))

# Plot countplots
for (col in categorical_features) {
  count_plot <- ggplot(ccard_cat, aes(x = !!sym(col), fill = factor(Defaulter))) +
    geom_bar(position = "dodge") +
    labs(title = paste("Countplot of", col),
         x = col,
         y = "Count",
         fill = "Defaulter") +
    theme_minimal()
  
  plot(count_plot)
}
```

Explore the history of repayment status: PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, and PAY_6
```{r}
PAY = c("PAY_0", "PAY_2", "PAY_3", "PAY_4", "PAY_5", "PAY_6")

for (i in PAY) {
  count_plot <- ggplot(ccard, aes(x = !!sym(i), fill = factor(default.payment.next.month))) +
    geom_bar(position = "dodge") +
    labs(title = paste("Countplot of", i),
         x = i,
         y = "Count",
         fill = "Defaulter") +
    theme_minimal()
  
  plot(count_plot)
}
```

Explore the amount of bill statement and previous payment for the past 6 months: 
```{r}
BILL_AMT = c("BILL_AMT1", "BILL_AMT2", "BILL_AMT3", "BILL_AMT4", "BILL_AMT5", "BILL_AMT6")
pairs(ccard[BILL_AMT])

PAY_AMT = c("PAY_AMT1", "PAY_AMT2", "PAY_AMT3", "PAY_AMT4", "PAY_AMT5", "PAY_AMT6")
pairs(ccard[PAY_AMT])
```

Check the correlation between variables
```{r}
library(corrplot)

data <- ccard
correlation_matrix <- cor(data)
corrplot(correlation_matrix)
```
As shown by the plot, the BILL_AMT variables are highly correlated. And, the closer the two months, the more correlated the payment status variables (i.e. PAY_2 and PAY_3). Therefore, in predictive analysis, we are going to use a new predictor to represent the BILL_AMT variables instead of including all of them in the model.

Explore the target variable default.payment.next.month
```{r}
freq <- table(default.payment.next.month)
barplot(freq, col = "steelblue", main = "Target Variable Distribution", ylim=c(0,30000), xlab = "Defaulter", ylab = "Frequency")
```


# 3. Feature Engineering

Rename the variables
```{r}
column_names <- list(PAY_0 = "PAY_SEPT", PAY_2 = "PAY_AUG", PAY_3 = "PAY_JUL", PAY_4 = "PAY_JUN", PAY_5 = "PAY_MAY", PAY_6 = "PAY_APR", BILL_AMT1 = "BILL_AMT_SEPT", BILL_AMT2 = "BILL_AMT_AUG", BILL_AMT3 = "BILL_AMT_JUL", BILL_AMT4 = "BILL_AMT_JUN", BILL_AMT5 = "BILL_AMT_MAY", BILL_AMT6 = "BILL_AMT_APR", PAY_AMT1 = "PAY_AMT_SEPT", PAY_AMT2 = "PAY_AMT_AUG", PAY_AMT3 = "PAY_AMT_JUL", PAY_AMT4 = "PAY_AMT_JUN", PAY_AMT5 = "PAY_AMT_MAY", PAY_AMT6 = "PAY_AMT_APR", default.payment.next.month = "IsDefaulter"
)

for (old_name in names(column_names)) {
  new_name <- column_names[[old_name]]
  names(ccard)[names(ccard) == old_name] <- new_name
}

names(ccard)
```

Combine and Delete features
```{r}
ccard1 = ccard
attach(ccard1)

# Combine EDUCATION = 0, 4 or 5 to 4
ccard1$EDUCATION = ifelse(ccard1$EDUCATION==0 | ccard1$EDUCATION==4 | ccard1$EDUCATION==5 |ccard1$EDUCATION==6, 4, ccard1$EDUCATION)

# Combine MARRIAGE = 0, 3 to 3
ccard1$MARRIAGE = ifelse(ccard1$MARRIAGE==0 | ccard1$MARRIAGE==3, 3, ccard1$MARRIAGE)

# Combine PAY_month = 4, 5, 6, 7, 8 to 4 for every month
ccard1$PAY_SEPT = ifelse(ccard1$PAY_SEPT==4 | ccard1$PAY_SEPT==5 |ccard1$PAY_SEPT==6 | ccard1$PAY_SEPT==7 | ccard1$PAY_SEPT==8, 4, ccard1$PAY_SEPT)
ccard1$PAY_AUG = ifelse(ccard1$PAY_AUG==4 | ccard1$PAY_AUG==5 |ccard1$PAY_AUG==6 | ccard1$PAY_AUG==7 | ccard1$PAY_AUG==8, 4, ccard1$PAY_AUG)
ccard1$PAY_JUL = ifelse(ccard1$PAY_JUL==4 | ccard1$PAY_JUL==5 |ccard1$PAY_JUL==6 | ccard1$PAY_JUL==7 | ccard1$PAY_JUL==8, 4, ccard1$PAY_JUL)
ccard1$PAY_JUN = ifelse(ccard1$PAY_JUN==4 | ccard1$PAY_JUN==5 |ccard1$PAY_JUN==6 | ccard1$PAY_JUN==7 | ccard1$PAY_JUN==8, 4, ccard1$PAY_JUN)
ccard1$PAY_MAY = ifelse(ccard1$PAY_MAY==4 | ccard1$PAY_MAY==5 |ccard1$PAY_MAY==6 | ccard1$PAY_MAY==7 | ccard1$PAY_MAY==8, 4, ccard1$PAY_MAY)
ccard1$PAY_APR = ifelse(ccard1$PAY_APR==4 | ccard1$PAY_APR==5 |ccard1$PAY_APR==6 | ccard1$PAY_APR==7 | ccard1$PAY_APR==8, 4, ccard1$PAY_APR)

# Delete PAY_AUG, PAY_JUL, PAY_JUN = 1 data #not sure???????????
ccard1 = subset(ccard1, !(PAY_AUG == 1 | PAY_JUL == 1 | PAY_JUN == 1))

# Delete irrelevant variable ID
ccard1 = subset(ccard1, select = -c(ID))
```

Convert categorical variables from int to factor
```{r}
categorical_vars <- c("SEX","EDUCATION","MARRIAGE","PAY_SEPT","PAY_AUG", "PAY_JUL", "PAY_JUN", "PAY_MAY", "PAY_APR", "IsDefaulter")
ccard1[categorical_vars] = lapply(ccard1[categorical_vars], as.factor)
str(ccard1)
```

Create a new variable AVG_BILL_AMT to replace highly correlated BILL_AMT features
```{r}
attach(ccard1)
ccard1$AVG_BILL_AMT = 1/6 * (BILL_AMT_SEPT + BILL_AMT_AUG + BILL_AMT_JUL + BILL_AMT_JUN + BILL_AMT_MAY + BILL_AMT_APR)
ccard1 = subset(ccard1, select = -c(BILL_AMT_SEPT, BILL_AMT_AUG, BILL_AMT_JUL, BILL_AMT_JUN, BILL_AMT_MAY, BILL_AMT_APR))
names(ccard1)
```

Handle missing data
```{r}
dim(ccard1)
ccard1 = na.omit(ccard1)
dim(ccard1)
```

Handle outliers
```{r}
# 3 sigma method

numeric_cols = sapply(ccard1, is.numeric)

valid_rows <- rep(TRUE, nrow(ccard1))

for (i in which(numeric_cols)) {
  mean_col <- mean(ccard1[, i])  
  sd_col <- sd(ccard1[, i])    
  lower_bound <- mean_col - 3 * sd_col
  upper_bound <- mean_col + 3 * sd_col
  # update valid_rows based on 3 sigma bound
  valid_rows <- valid_rows & (ccard1[, i] >= lower_bound & ccard1[, i] <= upper_bound)
}

ccard2 = ccard1[valid_rows,]

dim(ccard2)
```


# 4. Predictive Analysis

Perform train-test split
```{r}
set.seed(1)

train = sample(nrow(ccard2), nrow(ccard2)*0.7, replace = FALSE)
ccard.train = ccard2[train,]
ccard.test = ccard2[-train,]
```


4.1 Logistic Regression
```{r}
glm.fit = glm(IsDefaulter~., data = ccard.train, family = binomial)

glm.probs = predict(glm.fit, ccard.test, type = "response")

glm.pred = rep(0, nrow(ccard.test))
threshold = 0.5
glm.pred[glm.probs > threshold] = 1

table(glm.pred, ccard.test$IsDefaulter)
logiER = mean(glm.pred != ccard.test$IsDefaulter)
logiER
```


4.2. LDA
```{r}
library(MASS)

lda.fit = lda(IsDefaulter~., data = ccard.train)

lda.pred = predict(lda.fit, ccard.test)
lda.class = lda.pred$class

table(lda.class, ccard.test$IsDefaulter)
ldaER = mean(lda.class != ccard.test$IsDefaulter)
ldaER
```


4.3. QDA
```{r}
qda.fit = qda(IsDefaulter~., data = ccard.train)

qda.pred = predict(qda.fit, ccard.test)
qda.class = qda.pred$class

table(qda.class, ccard.test$IsDefaulter)
qdaER = mean(qda.class != ccard.test$IsDefaulter)
qdaER
```


4.4. Classification Tree
```{r}
library(tree)

tree.ccard = tree(IsDefaulter~., ccard.train)
plot(tree.ccard)
text(tree.ccard, pretty=0)

tree.pred = predict(tree.ccard, ccard.test, type="class")

table(tree.pred, ccard.test$IsDefaulter)
treeER = mean(tree.pred != ccard.test$IsDefaulter)
treeER
```


4.5. Bagging
```{r}
# Tuning the hyperparameter ntree using Grid search

library(randomForest)
set.seed(1)

testER = c()
ntreeList = seq(300,500,50)
for (ntree in ntreeList) {
  bag.ccard = randomForest(IsDefaulter~., data = ccard.train, mtry = ncol(ccard.train)-1, ntree = ntree, importance=FALSE)
  bag.pred = predict(bag.ccard, ccard.test)
  #print(mean(bag.pred != ccard.test$IsDefaulter))
  testER = append(testER, mean(bag.pred != ccard.test$IsDefaulter))
}
print(min(testER))
#plot(ntreeList, testER)
```


4.6. Random Forest

Tune the hyperparameter mtry using Grid search
```{r}
# usually mtry = sqrt(numPredictors) gives the best performance

set.seed(1)
testER = c()
mtryList = seq(2,6,1)
for (mtry in mtryList) {
  rf.ccard = randomForest(IsDefaulter~., data = ccard.train, mtry = mtry, ntree = bestntree, importance=FALSE)
  rf.pred = predict(rf.ccard, ccard.test)
  #print(mean(rf.pred != ccard.test$IsDefaulter))
  testER = append(testER, mean(rf.pred != ccard.test$IsDefaulter))
}
print(min(testER))
#plot(mtryList, testER)
```

```{r}
bestmtry = 4
rfER = min(testER)
```

Tune the hyperparameter ntree suing Grid Search
```{r}
set.seed(1)
testER = c()
ntreeList = seq(250,500,50)
for (ntree in ntreeList) {
  rf.ccard = randomForest(IsDefaulter~., data = ccard.train, mtry = bestmtry, ntree = ntree, importance=FALSE)
  rf.pred = predict(rf.ccard, ccard.test)
  #print(mean(rf.pred != ccard.test$IsDefaulter))
  testER = append(testER, mean(rf.pred != ccard.test$IsDefaulter))
}
print(min(testER))
#plot(ntreeList, testER)
```

```{r}
bestntree = 400
rfER = testER[4]
```

Build the best model, and show the importance of predictors
```{r}
set.seed(1)
rf.ccard = randomForest(IsDefaulter~., data = ccard.train, mtry = bestmtry, ntree = bestntree, importance=FALSE)
rf.pred = predict(rf.ccard, ccard.test)
rfER = mean(rf.pred != ccard.test$IsDefaulter)
rfER
  
varImpPlot(rf.ccard)

sorted_indices = order(importance(rf.ccard)[,1], decreasing = TRUE)
importance(rf.ccard)[sorted_indices, ]
```


4.7. Boosted Trees

Preprocessing the data
```{r}
library(xgboost)
set.seed(1)

# Convert categorical variables to numerical values -- Ordinal encoding
ccard3 = lapply(ccard2, function(x) { if(is.factor(x)) as.numeric(as.character(x)) else x})
ccard3 = data.frame(ccard3)

# One-hot encoding
# library(fastDummies)
# categorical_preds = c("SEX","EDUCATION","MARRIAGE","PAY_SEPT","PAY_AUG", "PAY_JUL", "PAY_JUN", "PAY_MAY", "PAY_APR")
# ccard3 = dummy_cols(ccard2, select_columns = categorical_preds, remove_first_dummy = TRUE) # remove the first level dummy to avoid multicollinearity
# ccard3 = ccard3[, !colnames(ccard3) %in% categorical_preds]
# colnames(ccard3) = make.names(colnames(ccard3)) # make valid names

# Recreate the train-test split
ccard.train = ccard3[train,]
ccard.test = ccard3[-train,]

x_train = ccard.train[, -which(names(ccard.train) == "IsDefaulter"), drop = FALSE]
x_train = as.matrix(sapply(x_train, as.numeric)) 
y_train = ccard.train$IsDefaulter
dtrain = xgb.DMatrix(data = x_train, label = y_train)

x_test = ccard.test[, -which(names(ccard.test) == "IsDefaulter"), drop = FALSE]
x_test = as.matrix(sapply(x_test, as.numeric))
y_test = ccard.test$IsDefaulter
dtest = xgb.DMatrix(data = x_test)
```

Tuning shrinkage
```{r}
# grid search with k-fold cross validation
shrinkage = seq(from = 0.1, to = 0.5, by = 0.1)

best_ER <- 1
best_eta <- NULL
for (eta in shrinkage) {
  set.seed(1)
  cv <- xgb.cv(
    data = dtrain,
    eta = eta, #
    objective = "binary:logistic",
    nfold = 5,
    metrics = "error",
    nrounds = 10,
    early_stopping_rounds = 10,
    verbose = FALSE
  )
      
  mean_ER = cv$evaluation_log$test_error_mean[length(cv$evaluation_log$test_error_mean)]
  if (mean_ER < best_ER) {
      best_ER = mean_ER
      best_eta = eta
  }
}
best_ER
best_eta
```

Tuning nrounds*
```{r}
ntreeList = seq(from = 4, to = 12, by = 2)

best_ER <- 1
best_ntree <- NULL
for (ntree in ntreeList) {
  set.seed(1)
  cv <- xgb.cv(
    data = dtrain,
    eta = best_eta,
    nrounds = ntree,
    objective = "binary:logistic",
    nfold = 5,
    metrics = "error",
    early_stopping_rounds = 10,
    verbose = FALSE
  )
      
  mean_ER = cv$evaluation_log$test_error_mean[length(cv$evaluation_log$test_error_mean)]
  if (mean_ER < best_ER) {
      best_ER = mean_ER
      best_ntree = ntree
  }
}
best_ER
best_ntree

boostER = best_ER
```


# 5. Model Evaluations

```{r}
# need Boosted tree and Neural network
models = c("Logistic Regression", "LDA", "QDA", "Tree", "Bagging", "Random Forest", "Boosted Trees")
errorRates = c(logiER, ldaER, qdaER, treeER, baggingER, rfER, boostER)
barplot(errorRates, names.arg = models, col = "steelblue", main = "Error Rates (log) by Model",
        xlab = "Model", ylab = "Error Rate (log)", log = "y", las=3)

errorRates
```

# 6. Conclusion

While most models demonstrate similar precision in predicting default cases, Logistic Regression, LDA, and decision trees outperform the others. The relationship between our listed predictors and the target is straightforward and easily interpretable, with the decision boundary between defaulters and non-defaulters being close to linear. 

The most significant predictors include the repayment amount and status from September, the average amount of the bill statement, age, and credit limit. Recent repayment information proves to be more predictive, with data from consecutive months showing high correlation. Moreover, clients who didn't pay on time usually made payments two months late, while only a small number paid just one month late.



